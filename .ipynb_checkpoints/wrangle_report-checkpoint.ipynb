{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling for WeRateDogs tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WeRateDogs](https://twitter.com/dog_rates) is popular twitter account for rating funny content (pictures and videos) with dogs. \n",
    "\n",
    "For analysis of tweets from the account I had **gathered data from 3 different sources (provided csv file, tsv file stored on URL and data from Twitter API)**. After I have assessed potentional **11 quality issues and 4 tidiness issues**, which were cleaned in last step of data wrangling process. This task was a part of online course [Data Analyst nanodegree from Udacity](https://www.udacity.com/course/data-analyst-nanodegree--nd002). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **WeRateDogs Twitter archive**: The csv file containing 2356 rows (each row for one tweet) and 17 columns was provided by Udacity to all students and I had to to only load csv file to Pandas dataframe.\n",
    "- **Tweet image predictions**: The tsv file included information about dog breed on image from tweet predicted by neural network. The file was stored on URL and I had to download it by using [Request library.](https://requests.readthedocs.io/en/master/0). Downloaded file had 2075 rows and 12 columns.\n",
    "- **Additional data about likes and retweets from Tweepy**: [Tweepy](http://docs.tweepy.org/en/latest/index.html) is Python library, where data about tweets are stored as **JSON objects**. To get data about count of likes and retweets for tweets I had to **access it via Twitter API**. First, it was necessary to create Twitter Developper acount. Then save JSON data to txt.file by using json.dump() method. Reading JSON string can be done either by method pd.read_json() or json.loads(). Additional data from Tweepy was gathered only for tweets from WeRate Twitter archive and we got 2331 rows of data, while 25 tweets from archive file were missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assessed data visually by printing few lines of each table using sample() method and then programatically by using methods like info(), value_counts(), duplicated() or any(). \n",
    "\n",
    "I came across various issues like one variable divided into four columns, mixed writting of lowercase and uppercase for one variable, inconvenient datatypes for further processing of data, nonsense dog names in name column, not fully visible text of tweet etc. \n",
    "\n",
    "Initially, I also thought that when **neural network predicted in all three predictions for one image, that there is no dog on picture, I can rely on this result** and delete these rows. First random check of 7 images where no dogs were predicted showed that it might be the case, because only on 1 of 7 I found a dog. Nevertheless, **later when doing same random check I was sometimes looking on 3 or 4 dogs out 7 images which were labeled by neural network as without dogs**. This made me rethink my previous approach and I kept all rows from Tweet image predictions, only narrowing three different predictions for one image to single prediction. I kept always prediction of dog breed with highest probability or NaN value, when no dog was predicted in either of 3 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues previously identified in assessing section were defined, provided solution by new code and tested if that solution worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling part took me much more time than later two sections, but it was useful practice to improve my data gathering skills. \n",
    "\n",
    "I also realized that visibility of content from all cells in dataframe should be addressed ideally during visual assessment phase of data wrangling. I have identified it as an issue for text of tweet and extended cells to display all content in cleaning phase. But only later when analysing data I found other valuable insights from different extended cells (info if tweet includes photo or video from expanded_urls column of WeRateDogs Twitter archive) which could have been included earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
